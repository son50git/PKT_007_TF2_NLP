{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords #provides list of english stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PRINT VERSION!!\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(pd.read_csv('ita.txt', sep='\\t',header = None, nrows=100000) , test_size=.10) #, nrows=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 2)\n",
      "(10000, 2)\n",
      "                   english                        italian\n",
      "22128      We're finished.            Noi abbiamo finito.\n",
      "40667    We won't be back.                   Non tornerà.\n",
      "52909   Whose is this bag?         Di chi è questa borsa?\n",
      "10199        We need help.  Noi abbiamo bisogno di aiuto.\n",
      "60073  It's been terrible.             È stato terribile.\n"
     ]
    }
   ],
   "source": [
    "train.columns = ['english','italian']\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['english_lower'] = train['english'].str.lower()\n",
    "train['english_no_punctuation'] = train['english_lower'].str.replace('[^\\w\\s]','')\n",
    "#train['english_no_stopwords'] = train['english_no_punctuation'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"].fillna(\"fillna\")\n",
    "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['italian_lower'] = train[\"italian\"].str.lower()\n",
    "train['italian_no_punctuation'] =  '_start_' + ' ' +train['italian_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**VERY IMPORTANT TRICK!! NOTICE THAT WE ADD \"_start_\" and \"_end_\" EXACTLY AT THE BEGINNING AND THE END OF EACH SENTENCE TO HAVE SOME KIND OF'DELIMITERS' THAT WILL TELL OUR DECODER TO START AND FINISH. BECAUSE WE DON'T HAVE GENERAL SIGNALS OF START AND FINISH IN NATURAL LANGUAGE. BASICALLY '_end_' REFLECTS THE POINT IN WHICH OUR OUTPUT SENTENCE IS MORE LIKELY TO END.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features1 = 5000\n",
    "maxlen1 = 15\n",
    "\n",
    "max_features2 = 5000\n",
    "maxlen2 = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok1 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features1) \n",
    "tok1.fit_on_texts(list(train['english_no_punctuation'])) #fit to cleaned text\n",
    "tf_train_english =tok1.texts_to_sequences(list(train['english_no_punctuation']))\n",
    "tf_train_english =tf.keras.preprocessing.sequence.pad_sequences(tf_train_english, maxlen=maxlen1) #let's execute pad step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the processing has to be done for both \n",
    "#two different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features2, filters = '*') \n",
    "tok2.fit_on_texts(list(train['italian_no_punctuation'])) #fit to cleaned text\n",
    "tf_train_italian = tok2.texts_to_sequences(list(train['italian_no_punctuation']))\n",
    "tf_train_italian = tf.keras.preprocessing.sequence.pad_sequences(tf_train_italian, maxlen=maxlen2, padding ='post') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder input: (90000, 14)\n",
      "Shape of decoder target: (90000, 14)\n",
      "Shape of encoder input: (90000, 15)\n"
     ]
    }
   ],
   "source": [
    "vectorized_italian = tf_train_italian\n",
    "# For Decoder Input, you don't need the last word as that is only for prediction\n",
    "# when we are training using Teacher Forcing.\n",
    "decoder_input_data = vectorized_italian[:, :-1]\n",
    "\n",
    "# Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
    "decoder_target_data = vectorized_italian[:, 1:]\n",
    "\n",
    "print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
    "print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
    "\n",
    "vectorized_english = tf_train_english\n",
    "# Encoder input is simply the body of the issue text\n",
    "encoder_input_data = vectorized_english\n",
    "doc_length = encoder_input_data.shape[1]\n",
    "print(f'Shape of encoder input: {encoder_input_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_encoder = len(tok1.word_index) + 1 #remember vocab size?\n",
    "vocab_size_decoder = len(tok1.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (English text)\n",
    "x = tf.keras.layers.Embedding(vocab_size_encoder, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "\n",
    "\n",
    "#Batch normalization is used so that the distribution of the inputs \n",
    "#to a specific layer doesn't change over time\n",
    "x = tf.keras.layers.BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = tf.keras.layers.GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = tf.keras.Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = tf.keras.Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (Italian text)\n",
    "dec_emb = tf.keras.layers.Embedding(vocab_size_decoder, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "#again batch normalization\n",
    "dec_bn = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out) #the decoder \"decodes\" the encoder output.\n",
    "x = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_size_decoder, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "seq2seq_Model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Examine Model Architecture Summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 40)     246400      Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 40)     160         Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 40)           256400      Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 40), ( 9840        Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 40)     160         Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 6160)   252560      Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 765,520\n",
      "Trainable params: 765,280\n",
      "Non-trainable params: 240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "#viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79200 samples, validate on 10800 samples\n",
      "79200/79200 [==============================] - 153s 2ms/sample - loss: 7.2920 - val_loss: 7.9467\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1200\n",
    "epochs = 1\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,  epochs=epochs,  validation_split=0.12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW WE HAVE ANOTHER IMPORTANT TIP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = ['I like food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Results On Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_len_title = 30\n",
    "# get the encoder's features for the decoder\n",
    "tok1.fit_on_texts(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokenized = tok1.texts_to_sequences(test_text)\n",
    "raw_tokenized = tf.keras.preprocessing.sequence.pad_sequences(raw_tokenized, maxlen=maxlen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_encoding = encoder_model.predict(raw_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = seq2seq_Model.get_layer('Decoder-Word-Embedding').output_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the input into the decoder\n",
    "decoder_inputs = seq2seq_Model.get_layer('Decoder-Input').input\n",
    "dec_emb = seq2seq_Model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
    "dec_bn = seq2seq_Model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
    "# Instead of setting the intial state from the encoder and forgetting about it, during inference\n",
    "# we are not doing teacher forcing, so we will have to have a feedback loop from predictions back into\n",
    "# the GRU, thus we define this input layer for the state so we can add this capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_inference_state_input = tf.keras.Input(shape=(latent_dim,), name='hidden_state_input')\n",
    "\n",
    "# we need to reuse the weights that is why we are getting this\n",
    "# If you inspect the decoder GRU that we created for training, it will take as input\n",
    "# 2 tensors -> (1) is the embedding layer output for the teacher forcing\n",
    "#                  (which will now be the last step's prediction, and will be _start_ on the first time step)\n",
    "#              (2) is the state, which we will initialize with the encoder on the first time step, but then\n",
    "#                   grab the state after the first prediction and feed that back in again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_out, gru_state_out = seq2seq_Model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct dense layers\n",
    "dec_bn2 = seq2seq_Model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
    "dense_out = seq2seq_Model.get_layer('Final-Output-Dense')(dec_bn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = tf.keras.Model([decoder_inputs, gru_inference_state_input],\n",
    "                          [dense_out, gru_state_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to save the encoder's embedding before its updated by decoder\n",
    "#   because we can use that as an embedding for other tasks.\n",
    "original_body_encoding = body_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value = np.array(tok2.word_index['_start_']).reshape(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentence = []\n",
    "stop_condition = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_inv = dict((v, k) for k, v in tok2.word_index.items())\n",
    "#vocabulary_inv[0] = \"<PAD/>\"\n",
    "#vocabulary_inv[1] = \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "io\n",
      "stato\n",
      "stato\n",
      "po\n",
      "po\n",
      "po\n",
      "po\n",
      "dei\n",
      "dei\n",
      "dei\n",
      "dei\n",
      "dei\n",
      "dei\n",
      "dei\n",
      "dei\n",
      "dei\n"
     ]
    }
   ],
   "source": [
    "while not stop_condition:\n",
    "    #print(1)\n",
    "    preds, st = decoder_model.predict([state_value, body_encoding])\n",
    "    #preds = preds[preds>0]\n",
    "    # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n",
    "    # Argmax will return the integer index corresponding to the\n",
    "    # prediction + 2 b/c we chopped off first two\n",
    "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
    "    #print(np.argmax(preds[:, :, 2:]))\n",
    "    # retrieve word from index prediction\n",
    "    #pred_word_str = tok.id2token[pred_idx]\n",
    "    pred_word_str = vocabulary_inv[pred_idx]\n",
    "    #print(pred_idx)\n",
    "    print(pred_word_str)\n",
    "    if pred_word_str == '_end_' or len(decoded_sentence) >= maxlen2:\n",
    "        stop_condition = True\n",
    "        break\n",
    "    decoded_sentence.append(pred_word_str)\n",
    "\n",
    "    # update the decoder for the next word\n",
    "    body_encoding = st\n",
    "    state_value = np.array(pred_idx).reshape(1, 1)\n",
    "    #print(state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {
    "height": "263px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
