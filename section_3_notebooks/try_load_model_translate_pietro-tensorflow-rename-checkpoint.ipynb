{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords #provides list of english stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(pd.read_csv('ita.txt', sep='\\t',header = None), test_size=.10) #, nrows=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289289, 2)\n",
      "(32144, 2)\n",
      "                                                 english  \\\n",
      "29389                                   Tom lives there.   \n",
      "121031                           Who are your neighbors?   \n",
      "310457  As far as I know, Tom doesn't have a girlfriend.   \n",
      "20542                                    They adore you.   \n",
      "44613                                 How can I do that?   \n",
      "\n",
      "                                           italian  \n",
      "29389                                Tom abita l√¨.  \n",
      "121031                     Chi sono le sue vicine?  \n",
      "310457  Per quel che so, Tom non ha una fidanzata.  \n",
      "20542                             Loro la adorano.  \n",
      "44613                            Come posso farlo?  \n"
     ]
    }
   ],
   "source": [
    "train.columns = ['english','italian']\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['english_lower'] = train['english'].str.lower()\n",
    "train['english_no_punctuation'] = train['english_lower'].str.replace('[^\\w\\s]','')\n",
    "#train['english_no_stopwords'] = train['english_no_punctuation'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"].fillna(\"fillna\")\n",
    "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['italian_lower'] = train[\"italian\"].str.lower()\n",
    "train['italian_no_punctuation'] =  '_start_' + ' ' +train['italian_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**VERY IMPORTANT TRICK!! NOTICE THAT WE ADD \"_start_\" and \"_end_\" EXACTLY AT THE BEGINNING AND THE END OF EACH SENTENCE TO HAVE SOME KIND OF'DELIMITERS' THAT WILL TELL OUR DECODER TO START AND FINISH. BECAUSE WE DON'T HAVE GENERAL SIGNALS OF START AND FINISH IN NATURAL LANGUAGE. BASICALLY '_end_' REFLECTS THE POINT IN WHICH OUR OUTPUT SENTENCE IS MORE LIKELY TO END.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features1 = 5000\n",
    "maxlen1 = 35\n",
    "\n",
    "max_features2 = 5000\n",
    "maxlen2 = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok1 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features1) \n",
    "tok1.fit_on_texts(list(train['english_no_punctuation'])) #fit to cleaned text\n",
    "tf_train_english =tok1.texts_to_sequences(list(train['english_no_punctuation']))\n",
    "tf_train_english =tf.keras.preprocessing.sequence.pad_sequences(tf_train_english, maxlen=maxlen1) #let's execute pad step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the processing has to be done for both \n",
    "#two different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok2 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features2, filters = '*') \n",
    "tok2.fit_on_texts(list(train['italian_no_punctuation'])) #fit to cleaned text\n",
    "tf_train_italian = tok2.texts_to_sequences(list(train['italian_no_punctuation']))\n",
    "tf_train_italian = tf.keras.preprocessing.sequence.pad_sequences(tf_train_italian, maxlen=maxlen2, padding ='post') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder input: (289289, 34)\n",
      "Shape of decoder target: (289289, 34)\n",
      "Shape of encoder input: (289289, 35)\n"
     ]
    }
   ],
   "source": [
    "vectorized_italian = tf_train_italian\n",
    "# For Decoder Input, you don't need the last word as that is only for prediction\n",
    "# when we are training using Teacher Forcing.\n",
    "decoder_input_data = vectorized_italian[:, :-1]\n",
    "\n",
    "# Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
    "decoder_target_data = vectorized_italian[:, 1:]\n",
    "\n",
    "print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
    "print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
    "\n",
    "vectorized_english = tf_train_english\n",
    "# Encoder input is simply the body of the issue text\n",
    "encoder_input_data = vectorized_english\n",
    "doc_length = encoder_input_data.shape[1]\n",
    "print(f'Shape of encoder input: {encoder_input_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size_encoder = len(tok1.word_index) + 1 #remember vocab size?\n",
    "vocab_size_decoder = len(tok1.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (English text)\n",
    "x = tf.keras.layers.Embedding(vocab_size_encoder, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "\n",
    "\n",
    "#Batch normalization is used so that the distribution of the inputs \n",
    "#to a specific layer doesn't change over time\n",
    "x = tf.keras.layers.BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = tf.keras.layers.GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = tf.keras.Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = tf.keras.Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (Italian text)\n",
    "dec_emb = tf.keras.layers.Embedding(vocab_size_decoder, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "#again batch normalization\n",
    "dec_bn = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out) #the decoder \"decodes\" the encoder output.\n",
    "x = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_size_decoder, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "seq2seq_Model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Examine Model Architecture Summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 40)     527200      Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 40)     160         Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 40)           537080      Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 40), ( 9720        Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 40)     160         Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 13180)  540380      Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,614,700\n",
      "Trainable params: 1,614,460\n",
      "Non-trainable params: 240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "#viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 254574 samples, validate on 34715 samples\n",
      "Epoch 1/6\n",
      "254574/254574 [==============================] - 4011s 16ms/step - loss: 4.5565 - val_loss: 3.5928\n",
      "Epoch 2/6\n",
      "254574/254574 [==============================] - 4006s 16ms/step - loss: 0.9371 - val_loss: 6.5202\n",
      "Epoch 3/6\n",
      "254574/254574 [==============================] - 4042s 16ms/step - loss: 0.6553 - val_loss: 2.8177\n",
      "Epoch 4/6\n",
      "254574/254574 [==============================] - 4009s 16ms/step - loss: 0.5225 - val_loss: 0.7521\n",
      "Epoch 5/6\n",
      "254574/254574 [==============================] - 4009s 16ms/step - loss: 0.4413 - val_loss: 0.4472\n",
      "Epoch 6/6\n",
      "254574/254574 [==============================] - 4003s 16ms/step - loss: 0.3852 - val_loss: 0.3905\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1200\n",
    "epochs = 6\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,  epochs=epochs,  validation_split=0.12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 40) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.save('seq2seq_full_data_6_epochs.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq_Model1 = seq2seq_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq_Model = tf.keras.models.load_model('seq2seq_subsample_1_epochs.h5')\n",
    "#seq2seq_Model = tf.keras.models.load_model('seq2seq_full_data_3_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW WE HAVE ANOTHER IMPORTANT TIP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text = ['I like food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Results On Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#max_len_title = 30\n",
    "# get the encoder's features for the decoder\n",
    "tok1.fit_on_texts(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_tokenized = tok1.texts_to_sequences(test_text)\n",
    "raw_tokenized = tf.keras.preprocessing.sequence.pad_sequences(raw_tokenized, maxlen=maxlen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_encoding = encoder_model.predict(raw_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = seq2seq_Model.get_layer('Decoder-Word-Embedding').output_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruct the input into the decoder\n",
    "decoder_inputs = seq2seq_Model.get_layer('Decoder-Input').input\n",
    "dec_emb = seq2seq_Model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
    "dec_bn = seq2seq_Model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
    "# Instead of setting the intial state from the encoder and forgetting about it, during inference\n",
    "# we are not doing teacher forcing, so we will have to have a feedback loop from predictions back into\n",
    "# the GRU, thus we define this input layer for the state so we can add this capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru_inference_state_input = tf.keras.Input(shape=(latent_dim,), name='hidden_state_input')\n",
    "\n",
    "# we need to reuse the weights that is why we are getting this\n",
    "# If you inspect the decoder GRU that we created for training, it will take as input\n",
    "# 2 tensors -> (1) is the embedding layer output for the teacher forcing\n",
    "#                  (which will now be the last step's prediction, and will be _start_ on the first time step)\n",
    "#              (2) is the state, which we will initialize with the encoder on the first time step, but then\n",
    "#                   grab the state after the first prediction and feed that back in again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru_out, gru_state_out = seq2seq_Model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruct dense layers\n",
    "dec_bn2 = seq2seq_Model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
    "dense_out = seq2seq_Model.get_layer('Final-Output-Dense')(dec_bn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_model = tf.keras.Model([decoder_inputs, gru_inference_state_input],\n",
    "                          [dense_out, gru_state_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to save the encoder's embedding before its updated by decoder\n",
    "#   because we can use that as an embedding for other tasks.\n",
    "original_body_encoding = body_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tok2.word_index.update({'_start_': 0})\n",
    "#tok2.word_index.update({'_end_':len(tok2.word_index)+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_value = np.array(tok2.word_index['_start_']).reshape(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_sentence = []\n",
    "stop_condition = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_inv = dict((v, k) for k, v in tok2.word_index.items())\n",
    "#vocabulary_inv[0] = \"<PAD/>\"\n",
    "#vocabulary_inv[1] = \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "me\n",
      "piace\n",
      "il\n",
      "caff√®\n",
      "_end_\n"
     ]
    }
   ],
   "source": [
    "while not stop_condition:\n",
    "    #print(1)\n",
    "    preds, st = decoder_model.predict([state_value, body_encoding])\n",
    "    #preds = preds[preds>0]\n",
    "    # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n",
    "    # Argmax will return the integer index corresponding to the\n",
    "    # prediction + 2 b/c we chopped off first two\n",
    "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
    "    #print(np.argmax(preds[:, :, 2:]))\n",
    "    # retrieve word from index prediction\n",
    "    #pred_word_str = tok.id2token[pred_idx]\n",
    "    pred_word_str = vocabulary_inv[pred_idx]\n",
    "    #print(pred_idx)\n",
    "    print(pred_word_str)\n",
    "    if pred_word_str == '_end_' or len(decoded_sentence) >= maxlen2:\n",
    "        stop_condition = True\n",
    "        break\n",
    "    decoded_sentence.append(pred_word_str)\n",
    "\n",
    "    # update the decoder for the next word\n",
    "    body_encoding = st\n",
    "    state_value = np.array(pred_idx).reshape(1, 1)\n",
    "    #print(state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {
    "height": "263px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
